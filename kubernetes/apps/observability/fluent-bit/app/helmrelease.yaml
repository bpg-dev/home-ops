---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: fluent-bit
spec:
  chartRef:
    kind: OCIRepository
    name: fluent-bit
  interval: 1h
  values:
    config:
      service: |-
        [SERVICE]
          daemon off
          flush {{ .Values.flush }}
          log_level {{ .Values.logLevel }}
          parsers_file /fluent-bit/etc/parsers.conf
          parsers_file /fluent-bit/etc/conf/custom_parsers.conf
          http_server on
          http_listen 0.0.0.0
          http_port {{ .Values.metricsPort }}
          health_check on
          # Buffer on-disk so brief Loki outages (e.g., during storage / node churn) don't drop logs.
          storage.path /var/log/fluent-bit/storage
          storage.sync normal
          storage.checksum off
          storage.max_chunks_up 128
          storage.backlog.mem_limit 25M

      inputs: |-
        [INPUT]
          name tail
          alias kubernetes
          path /var/log/containers/*.log
          # Use multiline parser to handle:
          # 1. Containerd partial lines (P/F flags)
          # 2. Java stacktraces and multiline log messages
          multiline.parser cri,multiline-java
          tag kubernetes.*
          skip_empty_lines on
          skip_long_lines on
          # Persist tail offsets and allow filesystem buffering.
          storage.type filesystem
          db /var/log/fluent-bit/tail-containers.db
          db.locking true

        [INPUT]
          name syslog
          alias proxmox
          mode udp
          listen 0.0.0.0
          port 5514
          # Proxmox forwards with RSYSLOG_SyslogProtocol23Format (RFC5424 / syslog protocol 23)
          parser syslog-rfc5424
          tag proxmox.*

      filters: |-
        [FILTER]
          name kubernetes
          match kubernetes.*
          # kube-apiserver responses (pod metadata) can exceed the default 32K; increase to avoid http_client buffer warnings
          buffer_size 128k
          merge_log on
          kube_tag_prefix kubernetes.var.log.containers.
          k8s-logging.parser on
          k8s-logging.exclude on
          namespace_labels off
          annotations off

        [FILTER]
          name nest
          match kubernetes.*
          operation lift
          nested_under kubernetes
          add_prefix k_

        [FILTER]
          name nest
          match kubernetes.*
          operation lift
          nested_under k_labels
          add_prefix k_labels_

        [FILTER]
          name modify
          match kubernetes.*
          rename k_labels_app.kubernetes.io/name app
          rename k_labels_app app
          rename k_labels_k8s-app app

        # ============================================================
        # Log Parsing via Lua
        # Tries patterns in order, stops at first match.
        # JSON logs are already handled by merge_log above.
        # ============================================================
        [FILTER]
          name lua
          match kubernetes.*
          script /fluent-bit/etc/conf/parse_logs.lua
          call parse_log

        # Normalize level field to lowercase for consistent querying
        [FILTER]
          name modify
          match kubernetes.*
          # Rename common alternatives to "level"
          rename severity level
          rename loglevel level
          rename lvl level

        [FILTER]
          name lua
          match kubernetes.*
          script /fluent-bit/etc/conf/parse_logs.lua
          call normalize_level

      customParsers: |-
        # Containerd log wrapper format
        [PARSER]
          name containerd
          format regex
          regex ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%L%z

        # Multiline parser for Java/Spring Boot stacktraces
        # Continues lines that are part of a stacktrace:
        # - Lines starting with whitespace (stacktrace frames)
        # - Lines starting with "Caused by:" or "Suppressed:"
        # - Lines starting with "... N more"
        # - Lines that are exception class names (java.*, javax.*, org.*, com.*, etc.)
        [MULTILINE_PARSER]
          name          multiline-java
          type          regex
          flush_timeout 1000
          # Start pattern: Spring Boot timestamp format or common log format
          # Matches: 2024-01-15T17:46:52.051-05:00  INFO ...
          rule          "start_state"   "/^(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}|[A-Z][a-z]{2} \d{2}).*/"   "cont"
          # Continuation: stacktrace lines (start with whitespace, "at ", "Caused by", etc.)
          rule          "cont"          "/^(\s+at\s|Caused by:|Suppressed:|\s*\.\.\.\s+\d+\s+more|\s+\.\.\.|\t|    |[a-z]+\.[a-zA-Z])/"   "cont"

      outputs: |-
        [OUTPUT]
          name loki
          match kubernetes.*
          host loki.observability
          port 3100
          line_format json
          # Retry long enough to ride out short Loki/PVC/CSI hiccups, but don't retry forever.
          Retry_Limit 60
          # Keep labels low-cardinality to support metricsâ†”logs correlation in Grafana.
          labels job=fluent-bit,namespace=$k_namespace_name,pod=$k_pod_name,container=$k_container_name,app=$app,stream=$stream

        [OUTPUT]
          name loki
          match proxmox.*
          host loki.observability
          port 3100
          line_format json
          # Retry long enough to ride out short Loki/PVC/CSI hiccups, but don't retry forever.
          Retry_Limit 60
          # syslog-rfc5424 parser emits `host` and `ident` fields.
          labels job=proxmox,host=$host,ident=$ident

      # Lua script for intelligent log parsing
      # Tries patterns in order, stops at first match
      extraFiles:
        parse_logs.lua: |
          -- Pattern definitions: tried in order, first match wins
          local patterns = {
            -- Spring Boot: 2024-01-15T17:46:54.848-05:00  INFO 12 --- [app] [thread] logger : message
            {
              name = "springboot",
              regex = "^(%d%d%d%d%-%d%d%-%d%dT%d%d:%d%d:%d%d%.%d+[%+%-]%d%d:%d%d)%s+(%w+)%s+(%d+)%s+%-%-%-%s+%[([^%]]+)%]%s+%[([^%]]*)%]%s+(%S+)%s+:%s+(.*)$",
              fields = {"time", "level", "pid", "appname", "thread", "logger", "message"}
            },
            -- Python Celery: [2024-01-15 22:43:03,853] [INFO] [logger] message
            {
              name = "python_celery",
              regex = "^%[(%d%d%d%d%-%d%d%-%d%d%s+%d%d:%d%d:%d%d,%d+)%]%s+%[(%w+)%]%s+%[([^%]]+)%]%s+(.*)$",
              fields = {"time", "level", "logger", "message"}
            },
            -- Zap logger (Go): timestamp\tLEVEL\tmodule\tsource\tmessage\t{json}
            {
              name = "zap",
              regex = "^(%d%d%d%d%-%d%d%-%d%dT%d%d:%d%d:%d%d%.%d+Z)\t(%w+)\t([^\t]+)\t([^\t]+)\t([^\t]+)(.*)$",
              fields = {"time", "level", "module", "source", "message", "extra"}
            },
            -- CoreDNS: [INFO] ip:port - id "query" RCODE flags size duration
            {
              name = "coredns",
              regex = "^%[(%w+)%]%s+([^%s:]+):(%d+)%s+%-%s+(%d+)%s+\"([^\"]+)\"%s+(%w+)%s+([^%s]+)%s+(%d+)%s+(%S+)$",
              fields = {"level", "client_ip", "client_port", "query_id", "query", "rcode", "flags", "size", "duration"}
            },
            -- Logfmt: time=X level=Y msg="Z" (common in Go apps like Cilium)
            {
              name = "logfmt",
              regex = "^time=(%S+)%s+level=(%w+)%s+msg=\"([^\"]+)\"(.*)$",
              fields = {"time", "level", "msg", "extra"}
            },
          }

          -- Patterns that indicate an error even if level is not set
          local error_indicators = {
            "Exception",
            "Error:",
            "FATAL",
            "panic:",
            "PANIC",
            "Caused by:",
            "Traceback",
            "at [%w%.]+%([%w%.]+%.java:%d+%)",  -- Java stacktrace frame
          }

          -- Parse log field using patterns
          function parse_log(tag, timestamp, record)
            local log = record["log"]
            if log == nil or log == "" then
              return 0, 0, 0
            end

            -- Skip if already parsed (has level field from merge_log JSON parsing)
            if record["level"] ~= nil then
              return 0, 0, 0
            end

            local modified = false

            -- Try each pattern
            for _, pattern in ipairs(patterns) do
              local captures = {string.match(log, pattern.regex)}
              if #captures > 0 then
                for i, field in ipairs(pattern.fields) do
                  if captures[i] and captures[i] ~= "" then
                    record[field] = captures[i]
                  end
                end
                record["_parser"] = pattern.name
                modified = true
                break
              end
            end

            -- Check for error indicators in the log message (for stacktraces, etc.)
            if record["level"] == nil or string.lower(record["level"] or "") == "info" then
              for _, indicator in ipairs(error_indicators) do
                if string.match(log, indicator) then
                  -- Only upgrade to error if we found an error indicator
                  -- and current level is not already error/fatal
                  local current = string.lower(record["level"] or "")
                  if current ~= "error" and current ~= "fatal" then
                    record["level"] = "error"
                    record["_error_detected"] = true
                    modified = true
                  end
                  break
                end
              end
            end

            if modified then
              return 1, timestamp, record
            end
            return 0, 0, 0
          end

          -- Normalize level to lowercase
          function normalize_level(tag, timestamp, record)
            local level = record["level"]
            if level == nil then
              return 0, 0, 0
            end

            local normalized = string.lower(level)
            -- Map WARNING to warn for consistency
            if normalized == "warning" then
              normalized = "warn"
            end

            if normalized ~= level then
              record["level"] = normalized
              return 1, timestamp, record
            end

            return 0, 0, 0
          end

    dashboards:
      enabled: true
    # Fluent Bit runs on every node; set requests/limits so resource dashboards can render and to avoid noisy eviction risk.
    # Keep CPU limits unset (house rule), but set a memory limit.
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        memory: 200Mi
    serviceMonitor:
      enabled: true
    logLevel: warn
