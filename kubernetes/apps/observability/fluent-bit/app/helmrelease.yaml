---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: fluent-bit
spec:
  chartRef:
    kind: OCIRepository
    name: fluent-bit
  interval: 1h
  values:
    config:
      service: |-
        [SERVICE]
          daemon off
          flush {{ .Values.flush }}
          log_level {{ .Values.logLevel }}
          parsers_file /fluent-bit/etc/parsers.conf
          parsers_file /fluent-bit/etc/conf/custom_parsers.conf
          http_server on
          http_listen 0.0.0.0
          http_port {{ .Values.metricsPort }}
          health_check on
          # Buffer on-disk so brief Loki outages (e.g., during storage / node churn) don't drop logs.
          storage.path /var/log/fluent-bit/storage
          storage.sync normal
          storage.checksum off
          storage.max_chunks_up 128
          storage.backlog.mem_limit 25M

      inputs: |-
        [INPUT]
          name tail
          alias kubernetes
          path /var/log/containers/*.log
          parser containerd
          tag kubernetes.*
          skip_empty_lines on
          skip_long_lines on
          # Persist tail offsets and allow filesystem buffering.
          storage.type filesystem
          db /var/log/fluent-bit/tail-containers.db
          db.locking true

        [INPUT]
          name syslog
          alias proxmox
          mode udp
          listen 0.0.0.0
          port 5514
          # Proxmox forwards with RSYSLOG_SyslogProtocol23Format (RFC5424 / syslog protocol 23)
          parser syslog-rfc5424
          tag proxmox.*

      filters: |-
        [FILTER]
          name kubernetes
          match kubernetes.*
          # kube-apiserver responses (pod metadata) can exceed the default 32K; increase to avoid http_client buffer warnings
          buffer_size 128k
          merge_log on
          kube_tag_prefix kubernetes.var.log.containers.
          k8s-logging.parser on
          k8s-logging.exclude on
          namespace_labels off
          annotations off

        [FILTER]
          name nest
          match kubernetes.*
          operation lift
          nested_under kubernetes
          add_prefix k_

        [FILTER]
          name nest
          match kubernetes.*
          operation lift
          nested_under k_labels
          add_prefix k_labels_

        [FILTER]
          name modify
          match kubernetes.*
          rename k_labels_app.kubernetes.io/name app
          rename k_labels_app app
          rename k_labels_k8s-app app

      customParsers: |-
        [PARSER]
          name containerd
          format regex
          regex ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%L%z

      outputs: |-
        [OUTPUT]
          name loki
          match kubernetes.*
          host loki.observability
          port 3100
          line_format json
          # Retry long enough to ride out short Loki/PVC/CSI hiccups, but don't retry forever.
          Retry_Limit 60
          # Keep labels low-cardinality to support metricsâ†”logs correlation in Grafana.
          labels job=fluent-bit,namespace=$k_namespace_name,pod=$k_pod_name,container=$k_container_name,app=$app,stream=$stream

        [OUTPUT]
          name loki
          match proxmox.*
          host loki.observability
          port 3100
          line_format json
          # Retry long enough to ride out short Loki/PVC/CSI hiccups, but don't retry forever.
          Retry_Limit 60
          # syslog-rfc5424 parser emits `host` and `ident` fields.
          labels job=proxmox,host=$host,ident=$ident
    dashboards:
      enabled: true
    # Fluent Bit runs on every node; set requests/limits so resource dashboards can render and to avoid noisy eviction risk.
    # Keep CPU limits unset (house rule), but set a memory limit.
    resources:
      requests:
        cpu: 50m
        memory: 100Mi
      limits:
        memory: 200Mi
    serviceMonitor:
      enabled: true
    logLevel: warn
